{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0667b45",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3743925167.py, line 524)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 524\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"=\" * 60\" ve been trained, evaluated, and the best model has been saved.\")\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle \n",
    "\n",
    "# Import necessary libraries from scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- CUDA Imports (cuML) Removed ---\n",
    "# NOTE: The attempt to install cuML failed (as expected for pip). \n",
    "# We are reverting to standard scikit-learn components for universal compatibility.\n",
    "# The code below will now use the standard Pipeline, TfidfVectorizer, and LinearSVC.\n",
    "# CUDA_ENABLED = False is implicit, and we rely entirely on CPU-based libraries.\n",
    "\n",
    "# Download necessary NLTK components (run this once)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "# --- Section 6.2: From Raw Data to Clean Text ---\n",
    "\n",
    "print(\"1. Loading Data...\")\n",
    "# 1. Load Data: The file is a zipped CSV, which pandas handles automatically.\n",
    "try:\n",
    "    df = pd.read_csv('https://files.consumerfinance.gov/ccdb/complaints.csv.zip')\n",
    "except Exception as e:\n",
    "    # If download fails, report a fatal error and re-raise.\n",
    "    print(f\"Fatal Error: Could not load data from URL. Please ensure you have internet access and the URL is correct. Error: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "print(\"2. Filtering and Cleaning...\")\n",
    "# 2. Filter and Clean\n",
    "# Filter the DataFrame to keep only the required columns and non-null narratives.\n",
    "df = df[['Product', 'Consumer complaint narrative']].copy()\n",
    "df.columns = ['Product', 'Narrative']\n",
    "\n",
    "# Drop rows with missing narratives\n",
    "df.dropna(subset=['Narrative'], inplace=True)\n",
    "\n",
    "# Define the required classification categories\n",
    "TARGET_CATEGORIES = [\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports',\n",
    "    'Debt collection',\n",
    "    'Consumer Loan',\n",
    "    'Mortgage'\n",
    "]\n",
    "\n",
    "# Filter the DataFrame to include only the target categories\n",
    "df = df[df['Product'].isin(TARGET_CATEGORIES)].copy()\n",
    "\n",
    "# Reduce category names for readability (optional but good practice)\n",
    "df['Product'] = df['Product'].replace({\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Report/Repair',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Consumer Loan': 'Consumer Loan',\n",
    "    'Mortgage': 'Mortgage'\n",
    "})\n",
    "\n",
    "# Display class balance\n",
    "print(f\"Total relevant complaints: {len(df)}\")\n",
    "print(\"Class distribution:\\n\", df['Product'].value_counts())\n",
    "\n",
    "\n",
    "# 3. Preprocess Text Function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Converts text to lowercase, removes punctuation/numbers, and stops.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation and numbers (keep only letters and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the Narrative column\n",
    "df['Clean_Narrative'] = df['Narrative'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# --- Section 6.3 & 6.4: Feature Engineering and Training ---\n",
    "\n",
    "print(\"\\n3. Splitting Data and Preparing Pipelines...\")\n",
    "# Define features (X) and target (y)\n",
    "X = df['Clean_Narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# 1. Split Data: Divide the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Create Pipelines and 3. Train Models\n",
    "# Define a single feature extractor (TF-IDF Vectorizer)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=5)\n",
    "\n",
    "# Define classifiers \n",
    "classifiers = {\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000, solver='liblinear'),\n",
    "    'LinearSVC': LinearSVC(random_state=42, dual=False) # Re-added dual=False for scikit-learn\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model in a pipeline\n",
    "best_accuracy = -1\n",
    "for name, classifier in classifiers.items():\n",
    "    print(f\"   -> Training {name}...\")\n",
    "    \n",
    "    # Create the standard scikit-learn Pipeline\n",
    "    current_pipeline = Pipeline([\n",
    "        ('tfidf', tfidf),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    # Train the pipeline\n",
    "    current_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = current_pipeline.predict(X_test)\n",
    "    \n",
    "    # No conversion needed as y_pred is already a numpy array/pandas series\n",
    "    \n",
    "    # Evaluate performance\n",
    "    report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results for comparison table\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': round(accuracy, 4),\n",
    "        'Precision (macro)': round(report['macro avg']['precision'], 4),\n",
    "        'Recall (macro)': round(report['macro avg']['recall'], 4),\n",
    "        'F1-Score (macro)': round(report['macro avg']['f1-score'], 4)\n",
    "    })\n",
    "    \n",
    "    # Check if this is the best model based on accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model_name = name\n",
    "        best_model_pipeline = current_pipeline\n",
    "        best_y_pred = y_pred\n",
    "\n",
    "\n",
    "# 4. Compare Performance: Table 4\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\n4. Model Performance Comparison (Table 4):\")\n",
    "print(comparison_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "# --- Section 6.5: Evaluating Model Performance (Best Model) ---\n",
    "\n",
    "print(f\"\\n5. Detailed Evaluation of the Best Model ({best_model_name})...\")\n",
    "\n",
    "# 1. Select Best Model (Dynamically selected based on highest accuracy)\n",
    "# best_model_pipeline holds the best trained pipeline.\n",
    "\n",
    "# 2. Classification Report\n",
    "print(\"\\nClassification Report (Per Category Breakdown):\")\n",
    "print(classification_report(y_test, best_y_pred, target_names=y.unique(), zero_division=0))\n",
    "\n",
    "\n",
    "# 3. Confusion Matrix\n",
    "cm = confusion_matrix(y_test, best_y_pred, labels=y.unique())\n",
    "cm_df = pd.DataFrame(cm, index=y.unique(), columns=y.unique())\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black')\n",
    "plt.title(f'Confusion Matrix for {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# 4. Model Persistence: Save the best model pipeline\n",
    "MODEL_FILENAME = 'best_text_classifier_pipeline.pkl'\n",
    "try:\n",
    "    with open(MODEL_FILENAME, 'wb') as file:\n",
    "        pickle.dump(best_model_pipeline, file)\n",
    "    print(f\"\\nModel Saved Successfully: The best model ({best_model_name} Pipeline) was saved to {MODEL_FILENAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saving model: {e}\")\n",
    "\n",
    "print(\"\\nTask 5 Complete: Classification models ha# Cell 1: Check CUDA and Import Libraries\")\n",
    "print(\"=\" * 60)\n",
    "print(\"CUDA-Accelerated Text Classification\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Check CUDA availability\n",
    "try:\n",
    "    import cupy as cp\n",
    "    print(f\"✓ CUDA Available: {cp.cuda.is_available()}\")\n",
    "    if cp.cuda.is_available():\n",
    "        print(f\"✓ GPU Device: {cp.cuda.Device().name.decode()}\")\n",
    "        print(f\"✓ GPU Memory: {cp.cuda.Device().mem_info[1] / 1e9:.2f} GB total\")\n",
    "    CUDA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"✗ CuPy not installed. Please install RAPIDS cuML.\")\n",
    "    print(\"  Install with: conda install -c rapidsai -c conda-forge -c nvidia cuml cupy\")\n",
    "    CUDA_AVAILABLE = False\n",
    "\n",
    "print(\"\\nImporting libraries...\")\n",
    "\n",
    "# Import cuML (RAPIDS) for GPU\n",
    "if CUDA_AVAILABLE:\n",
    "    from cuml.naive_bayes import MultinomialNB as cuMultinomialNB\n",
    "    from cuml.linear_model import LogisticRegression as cuLogisticRegression\n",
    "    from cuml.svm import LinearSVC as cuLinearSVC\n",
    "    print(\"✓ cuML imported successfully\")\n",
    "\n",
    "# Import scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Download NLTK stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"✓ NLTK stopwords available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "print(\"\\n✓ All libraries imported successfully!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 2: Load and Filter Data\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADING AND FILTERING DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n📥 Downloading dataset from Consumer Finance...\")\n",
    "try:\n",
    "    df = pd.read_csv('https://files.consumerfinance.gov/ccdb/complaints.csv.zip')\n",
    "    print(f\"✓ Dataset loaded: {len(df):,} total complaints\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Filter columns\n",
    "df = df[['Product', 'Consumer complaint narrative']].copy()\n",
    "df.columns = ['Product', 'Narrative']\n",
    "\n",
    "print(f\"\\n🔍 Filtering data...\")\n",
    "print(f\"   Before filtering: {len(df):,} rows\")\n",
    "\n",
    "# Drop missing narratives\n",
    "df.dropna(subset=['Narrative'], inplace=True)\n",
    "print(f\"   After dropping nulls: {len(df):,} rows\")\n",
    "\n",
    "# Define target categories\n",
    "TARGET_CATEGORIES = [\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports',\n",
    "    'Debt collection',\n",
    "    'Consumer Loan',\n",
    "    'Mortgage'\n",
    "]\n",
    "\n",
    "df = df[df['Product'].isin(TARGET_CATEGORIES)].copy()\n",
    "print(f\"   After category filter: {len(df):,} rows\")\n",
    "\n",
    "# Simplify category names\n",
    "df['Product'] = df['Product'].replace({\n",
    "    'Credit reporting, credit repair services, or other personal consumer reports': 'Credit Report/Repair',\n",
    "    'Debt collection': 'Debt Collection',\n",
    "    'Consumer Loan': 'Consumer Loan',\n",
    "    'Mortgage': 'Mortgage'\n",
    "})\n",
    "\n",
    "print(\"\\n📊 Class Distribution:\")\n",
    "display(df['Product'].value_counts().to_frame('Count'))\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 3: Text Preprocessing\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TEXT PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Converts text to lowercase, removes punctuation/numbers, and stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "\n",
    "print(\"\\n🔧 Cleaning text data...\")\n",
    "print(\"   Operations: lowercase → remove punctuation → remove stopwords\")\n",
    "\n",
    "# Show example before/after\n",
    "sample_idx = 0\n",
    "print(f\"\\n📝 Example transformation:\")\n",
    "print(f\"   BEFORE: {df['Narrative'].iloc[sample_idx][:150]}...\")\n",
    "df['Clean_Narrative'] = df['Narrative'].apply(preprocess_text)\n",
    "print(f\"   AFTER:  {df['Clean_Narrative'].iloc[sample_idx][:150]}...\")\n",
    "\n",
    "print(f\"\\n✓ Preprocessing complete!\")\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 4: Prepare Data for GPU Training\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARING DATA FOR GPU TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "X = df['Clean_Narrative']\n",
    "y = df['Product']\n",
    "\n",
    "# Split data\n",
    "print(\"\\n✂️  Splitting data (75% train, 25% test)...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"   Training samples: {len(X_train):,}\")\n",
    "print(f\"   Testing samples:  {len(X_test):,}\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\n🔤 Vectorizing text with TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=5)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "print(f\"   Vocabulary size: {len(tfidf.vocabulary_):,} features\")\n",
    "print(f\"   Train matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"   Test matrix shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Convert to dense and transfer to GPU\n",
    "print(\"\\n🚀 Transferring data to GPU...\")\n",
    "X_train_dense = X_train_tfidf.toarray()\n",
    "X_test_dense = X_test_tfidf.toarray()\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    X_train_gpu = cp.asarray(X_train_dense, dtype=cp.float32)\n",
    "    X_test_gpu = cp.asarray(X_test_dense, dtype=cp.float32)\n",
    "    \n",
    "    gpu_memory_train = X_train_gpu.nbytes / 1e6\n",
    "    gpu_memory_test = X_test_gpu.nbytes / 1e6\n",
    "    print(f\"   GPU memory used (train): {gpu_memory_train:.2f} MB\")\n",
    "    print(f\"   GPU memory used (test):  {gpu_memory_test:.2f} MB\")\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    y_train_gpu = cp.asarray(y_train_encoded, dtype=cp.int32)\n",
    "    y_test_gpu = cp.asarray(y_test_encoded, dtype=cp.int32)\n",
    "    \n",
    "    print(f\"   Classes: {list(le.classes_)}\")\n",
    "    print(\"\\n✓ Data ready for GPU training!\")\n",
    "else:\n",
    "    print(\"✗ CUDA not available, cannot proceed with GPU training\")\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 5: Train GPU-Accelerated Models\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING GPU-ACCELERATED MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if not CUDA_AVAILABLE:\n",
    "    print(\"✗ Skipping training - CUDA not available\")\n",
    "else:\n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'MultinomialNB': cuMultinomialNB(),\n",
    "        'LogisticRegression': cuLogisticRegression(max_iter=1000),\n",
    "        'LinearSVC': cuLinearSVC(max_iter=1000)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    best_accuracy = -1\n",
    "    trained_models = {}\n",
    "    \n",
    "    # Train each model\n",
    "    for name, classifier in classifiers.items():\n",
    "        print(f\"\\n🔄 Training {name} on GPU...\")\n",
    "        \n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train\n",
    "        classifier.fit(X_train_gpu, y_train_gpu)\n",
    "        train_time = time.time() - start_time\n",
    "        \n",
    "        # Predict\n",
    "        y_pred_gpu = classifier.predict(X_test_gpu)\n",
    "        \n",
    "        # Transfer back to CPU\n",
    "        y_pred_cpu = cp.asnumpy(y_pred_gpu).astype(int)\n",
    "        y_pred = le.inverse_transform(y_pred_cpu)\n",
    "        \n",
    "        # Evaluate\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"   ✓ Training time: {train_time:.2f}s\")\n",
    "        print(f\"   ✓ Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Training Time (s)': round(train_time, 2),\n",
    "            'Accuracy': round(accuracy, 4),\n",
    "            'Precision (macro)': round(report['macro avg']['precision'], 4),\n",
    "            'Recall (macro)': round(report['macro avg']['recall'], 4),\n",
    "            'F1-Score (macro)': round(report['macro avg']['f1-score'], 4)\n",
    "        })\n",
    "        \n",
    "        # Store model and predictions\n",
    "        trained_models[name] = {\n",
    "            'model': classifier,\n",
    "            'predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        # Track best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_name = name\n",
    "            best_model = classifier\n",
    "            best_y_pred = y_pred\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "    comparison_df = pd.DataFrame(results)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    print(f\"\\n🏆 Best Model: {best_model_name} (Accuracy: {best_accuracy:.4f})\")\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 6: Detailed Evaluation of Best Model\n",
    "if CUDA_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"DETAILED EVALUATION: {best_model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_test, best_y_pred, target_names=le.classes_, zero_division=0))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(\"\\n📊 Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, best_y_pred, labels=le.classes_)\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    \n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(\"\\n📈 Per-Class Accuracy:\")\n",
    "    class_accuracies = []\n",
    "    for i, class_name in enumerate(le.classes_):\n",
    "        class_acc = cm[i, i] / cm[i, :].sum()\n",
    "        class_accuracies.append({\n",
    "            'Class': class_name,\n",
    "            'Accuracy': f\"{class_acc:.4f}\",\n",
    "            'Support': cm[i, :].sum()\n",
    "        })\n",
    "    display(pd.DataFrame(class_accuracies))\n",
    "\n",
    "# ============================================================================\n",
    "# Cell 7: Save Model\n",
    "if CUDA_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAVING MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    MODEL_FILENAME = 'best_cuda_classifier.pkl'\n",
    "    \n",
    "    try:\n",
    "        model_package = {\n",
    "            'model': best_model,\n",
    "            'vectorizer': tfidf,\n",
    "            'label_encoder': le,\n",
    "            'model_name': best_model_name,\n",
    "            'accuracy': best_accuracy,\n",
    "            'classes': list(le.classes_)\n",
    "        }\n",
    "        \n",
    "        with open(MODEL_FILENAME, 'wb') as file:\n",
    "            pickle.dump(model_package, file)\n",
    "        \n",
    "        print(f\"✓ Model saved successfully to '{MODEL_FILENAME}'\")\n",
    "        print(f\"  Model: {best_model_name}\")\n",
    "        print(f\"  Accuracy: {best_accuracy:.4f}\")\n",
    "        print(f\"  Classes: {list(le.classes_)}\")\n",
    "        \n",
    "        # Show how to load\n",
    "        print(\"\\n📖 To load the model later:\")\n",
    "        print(\"   with open('best_cuda_classifier.pkl', 'rb') as f:\")\n",
    "        print(\"       model_package = pickle.load(f)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error saving model: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"✓ ALL TASKS COMPLETE!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8be6385c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (676106818.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[7], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    conda install -c rapidsai -c conda-forge -c nvidia \\\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# For CUDA 11.x\n",
    "conda install -c rapidsai -c conda-forge -c nvidia \\\n",
    "    cuml cupy cudatoolkit=11.8\n",
    "\n",
    "# OR for CUDA 12.x\n",
    "conda install -c rapidsai -c conda-forge -c nvidia \\\n",
    "    cuml cupy cuda-version=12.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
